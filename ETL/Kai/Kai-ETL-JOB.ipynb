{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"\n",
						"Installed kernel version: 1.0.5 \n"
					]
				}
			],
			"source": [
				"# %help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.5 \n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 20 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 2\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 2\n",
						"Idle Timeout: 20\n",
						"Session ID: 7d9c6042-c161-42de-9b8b-60efab9ffa6e\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.5\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session 7d9c6042-c161-42de-9b8b-60efab9ffa6e to get into ready status...\n",
						"Session 7d9c6042-c161-42de-9b8b-60efab9ffa6e has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"%idle_timeout 200\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# important! using python min, max won't work\n",
				"from pyspark.sql.functions import col, min, max, sum, avg, count, countDistinct, row_number\n",
				"from pyspark.sql.window import Window\n",
				"\n",
				"# https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
				"from pyspark.sql.types import StructType, StructField, BooleanType, ByteType, ShortType, IntegerType, StringType, FloatType, DoubleType"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## aisles\n",
				"read as csv, save as parquet, then read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						" |-- aisle_id: integer (nullable = true)\n",
						" |-- aisle: string (nullable = true)\n",
						"\n",
						"row count: 134\n"
					]
				}
			],
			"source": [
				"aisles_schema = StructType([\n",
				"    StructField(\"aisle_id\", IntegerType(), True),\n",
				"    StructField(\"aisle\", StringType(), True)\n",
				"])\n",
				"\n",
				"# read from csv in s3\n",
				"s3_input_path = \"s3://weikaibucket/data/aisles/aisles.csv\"\n",
				"aisles = spark.read.csv(s3_input_path, header=True, schema=aisles_schema)\n",
				"\n",
				"# save as parquet\n",
				"s3_output_path = \"s3://weikaibucket/data_parquet/aisles/\"\n",
				"aisles.write.mode(\"overwrite\").parquet(s3_output_path)\n",
				"\n",
				"# read parquet\n",
				"aisles = spark.read.parquet(s3_output_path)\n",
				"\n",
				"aisles.printSchema()\n",
				"print(f'row count: {aisles.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## departments\n",
				"read as csv, save as parquet, then read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						" |-- department_id: integer (nullable = true)\n",
						" |-- department: string (nullable = true)\n",
						"\n",
						"row count: 21\n"
					]
				}
			],
			"source": [
				"departments_schema = StructType([\n",
				"    StructField(\"department_id\", IntegerType(), True),\n",
				"    StructField(\"department\", StringType(), True)\n",
				"])\n",
				"\n",
				"# read from csv in s3\n",
				"departments = spark.read.csv(\"s3://weikaibucket/data/departments/departments.csv\", header=True, schema=departments_schema)\n",
				"\n",
				"# save as parquet\n",
				"departments.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/departments/\")\n",
				"\n",
				"# read parquet\n",
				"departments = spark.read.parquet('s3://weikaibucket/data_parquet/departments')\n",
				"\n",
				"\n",
				"departments.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {departments.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## products\n",
				"read as csv, save as parquet, then read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						" |-- product_id: integer (nullable = true)\n",
						" |-- product_name: string (nullable = true)\n",
						" |-- aisle_id: integer (nullable = true)\n",
						" |-- department_id: integer (nullable = true)\n",
						"\n",
						"row count: 49688\n"
					]
				}
			],
			"source": [
				"products_schema = StructType([\n",
				"    StructField(\"product_id\", IntegerType(), True),\n",
				"    StructField(\"product_name\", StringType(), True),\n",
				"    StructField(\"aisle_id\", IntegerType(), True),\n",
				"    StructField(\"department_id\", IntegerType(), True)\n",
				"])\n",
				"# read from csv in s3\n",
				"products = spark.read.csv(\"s3://weikaibucket/data/products/products.csv\", header=True, schema=products_schema)\n",
				"\n",
				"# save as parquet\n",
				"products.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/aisles_parquet/\")\n",
				"\n",
				"# read parquet\n",
				"products = spark.read.parquet('s3://weikaibucket/data_parquet/aisles_parquet/')\n",
				"\n",
				"\n",
				"products.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {products.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## orders\n",
				"read as csv, partition by eval_set, save as parquet, then read from parque"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						" |-- order_id: integer (nullable = true)\n",
						" |-- user_id: integer (nullable = true)\n",
						" |-- order_number: integer (nullable = true)\n",
						" |-- order_dow: byte (nullable = true)\n",
						" |-- order_hour_of_day: byte (nullable = true)\n",
						" |-- days_since_prior_order: float (nullable = true)\n",
						" |-- eval_set: string (nullable = true)\n",
						"\n",
						"row count: 3421083\n",
						"+-----------------+-----------------+\n",
						"|min(order_number)|max(order_number)|\n",
						"+-----------------+-----------------+\n",
						"|                1|              100|\n",
						"+-----------------+-----------------+\n",
						"\n",
						"+---------------------------+---------------------------+\n",
						"|min(days_since_prior_order)|max(days_since_prior_order)|\n",
						"+---------------------------+---------------------------+\n",
						"|                        0.0|                       30.0|\n",
						"+---------------------------+---------------------------+\n"
					]
				}
			],
			"source": [
				"orders_schema = StructType([\n",
				"    StructField(\"order_id\", IntegerType(), True),\n",
				"    StructField(\"user_id\", IntegerType(), True),\n",
				"    StructField(\"eval_set\", StringType(), True),\n",
				"    StructField(\"order_number\", IntegerType(), True),\n",
				"    StructField(\"order_dow\", ByteType(), True),\n",
				"    StructField(\"order_hour_of_day\", ByteType(), True),\n",
				"    StructField(\"days_since_prior_order\", FloatType(), True)\n",
				"])\n",
				"# read from csv in s3\n",
				"orders = spark.read.csv(\"s3://weikaibucket/data/orders/orders.csv\", header=True, schema=products_schema)\n",
				"\n",
				"# save as parquet\n",
				"orders.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/orders/\")\n",
				"\n",
				"# read parquet\n",
				"orders = spark.read.parquet('s3://weikaibucket/data_parquet/orders')\n",
				"\n",
				"\n",
				"orders.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {orders.count()}')\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## order_products\n",
				"read as csv, save as parquet, then read from parque"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						" |-- order_id: integer (nullable = true)\n",
						" |-- product_id: integer (nullable = true)\n",
						" |-- add_to_cart_order: integer (nullable = true)\n",
						" |-- reordered: boolean (nullable = true)\n",
						"\n",
						"row count: 33819106\n"
					]
				}
			],
			"source": [
				"# takes 1 minute to run\n",
				"order_products_schema = StructType([\n",
				"    StructField(\"order_id\", IntegerType(), True),\n",
				"    StructField(\"product_id\", IntegerType(), True),\n",
				"    StructField(\"add_to_cart_order\", IntegerType(), True),\n",
				"    StructField(\"reordered\", IntegerType(), True)\n",
				"])\n",
				"order_products = spark.read.csv(\"s3://weikaibucket/data/order_products/\", header=True, schema=products_schema)\n",
				"\n",
				"# save as parquet\n",
				"order_products.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/order_products/\")\n",
				"\n",
				"# read parquet\n",
				"order_products = spark.read.parquet('s3://weikaibucket/data_parquet/order_products')\n",
				"\n",
				"\n",
				"order_products.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {order_products.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## order_products_prior\n",
				"already save as parquet, just read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# read parquet\n",
				"order_products_prior = spark.read.parquet('s3://weikaibucket/features/order_products_prior/')\n",
				"\n",
				"\n",
				"order_products_prior.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {order_products_prior.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q2\n",
				"```sql\n",
				"CREATE TABLE user_features_1 WITH (\n",
				"\texternal_location = 's3://weikaibucket/features/user_features_1/',\n",
				"\tformat = 'PARQUET'\n",
				") AS\n",
				"SELECT user_id,\n",
				"\tMAX(order_number) AS max_order_number,\n",
				"\tSUM(days_since_prior_order) AS sum_days_since_prior_order,\n",
				"\tAVG(days_since_prior_order) AS avg_days_since_prior_order\n",
				"FROM orders\n",
				"GROUP BY user_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"user_features_1 = spark.read.parquet('s3://weikaibucket/features/user_features_1/') # read as parquet\n",
				"\n",
				"user_features_1.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {user_features_1.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q3\n",
				"```sql\n",
				"Create Table user_features_2 with (\n",
				"\texternal_location = 's3://weikaibucket/features/user_features_2/',\n",
				"\tformat = 'parquet'\n",
				") as\n",
				"select user_id,\n",
				"\tcount(product_id) as total_number_of_products,\n",
				"\tcount(Distinct product_id) as total_number_of_distinct_products,\n",
				"\t--整数除法。如果结果是一个小于 1 的小数，那么结果会被截断为 0,所以要乘以1.0变浮点数\n",
				"\tsum(if(reordered=1,1,0)) * 1.0 / sum(if(order_number>1,order_number,null)) as reorder_ratio\n",
				"from order_products_prior\n",
				"group by user_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"user_features_2 = spark.read.parquet('s3://weikaibucket/features/user_features_2/') # read as parquet\n",
				"\n",
				"user_features_2.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {user_features_2.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q4\n",
				"```sql\n",
				"Create Table up_features with (\n",
				"\texternal_location = 's3://weikaibucket/features/up_features/',\n",
				"\tformat = 'parquet'\n",
				") as\n",
				"select user_id,\n",
				"\tproduct_id,\n",
				"\tcount(order_id) as total_number_of_orders,\n",
				"\tmin(order_number) as minimum_order_number,\n",
				"\tmax(order_number) as max_order_number,\n",
				"\tavg(add_to_cart_order) as avg_add_to_cart_order\n",
				"from order_products_prior\n",
				"group by user_id,\n",
				"\tproduct_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+-------+----------+------------+----------------+----------------+---------------------+\n",
						"|user_id|product_id|total_orders|min_order_number|max_order_number|avg_add_to_cart_order|\n",
						"+-------+----------+------------+----------------+----------------+---------------------+\n",
						"|      1|       196|          10|               1|              10|                  1.4|\n",
						"|      1|     10258|           9|               2|              10|   3.3333333333333335|\n",
						"|      1|     10326|           1|               5|               5|                  5.0|\n",
						"|      1|     12427|          10|               1|              10|                  3.3|\n",
						"|      1|     13032|           3|               2|              10|    6.333333333333333|\n",
						"+-------+----------+------------+----------------+----------------+---------------------+\n",
						"only showing top 5 rows\n",
						"\n",
						"row count: 13307953\n"
					]
				}
			],
			"source": [
				"up_features = spark.read.parquet('s3://weikaibucket/features/up_features/') # read as parquet\n",
				"\n",
				"up_features.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {up_features.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q5\n",
				"```sql\n",
				"Create Table prd_features with (\n",
				"\texternal_location = 's3://weikaibucket/features/prd_features/',\n",
				"\tformat = 'parquet'\n",
				") as\n",
				"select product_id,\n",
				"\tcount(product_id) as count_product,\n",
				"\tsum(reordered) as sum_of_reordered,\n",
				"\tcount(\n",
				"\t\tif (product_seq_table.product_seq_time = 1, 1, null)\n",
				"\t) as seq_time1,\n",
				"\tcount(\n",
				"\t\tif (product_seq_table.product_seq_time = 2, 1, null)\n",
				"\t) as seq_time2\n",
				"from (\n",
				"\t\tselect user_id,\n",
				"\t\t\tproduct_id,\n",
				"\t\t\treordered,\n",
				"\t\t\trow_number() over (\n",
				"\t\t\t\tpartition by user_id,\n",
				"\t\t\t\tproduct_id\n",
				"\t\t\t\torder by order_number Asc\n",
				"\t\t\t) as product_seq_time\n",
				"\t\tfrom order_products_prior\n",
				"\t) as product_seq_table\n",
				"group by product_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"prd_features = spark.read.parquet('s3://weikaibucket/features/prd_features/') # read as parquet\n",
				"\n",
				"prd_features.printSchema()\n",
				"\n",
				"# print row number\n",
				"print(f'Row count: {prd_features.count()}')"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
