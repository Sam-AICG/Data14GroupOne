{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# AWS Glue Studio Notebook\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "####  Run this cell to set up and start your interactive session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%timeout 15\n",
    "%idle_timeout 15\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Run local test with pySpark) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/10 00:47:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Parquet\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aisles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aisle_id: integer (nullable = true)\n",
      " |-- aisle: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# File path to the input CSV file\n",
    "csv_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/aisles/\"\n",
    "\n",
    "# Read the CSV file with schema inference\n",
    "aisles_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# \"overwrite\" ensures that any existing data at the specified output Parquet file path \n",
    "# is overwritten with the new data from the DataFrame\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/aisles/aisles.parquet\"\n",
    "aisles_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "aisles_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(parquet_file_path)\n",
    "\n",
    "aisles_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**departments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File path to the input CSV file\n",
    "csv_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/departments/\"\n",
    "\n",
    "# Read the CSV file with schema inference\n",
    "departments_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# \"overwrite\" ensures that any existing data at the specified output Parquet file path \n",
    "# is overwritten with the new data from the DataFrame\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/departments/departments.parquet\"\n",
    "departments_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "departments_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(parquet_file_path)\n",
    "\n",
    "departments_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 00:48:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 10:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- eval_set: string (nullable = true)\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- order_dow: integer (nullable = true)\n",
      " |-- order_hour_of_day: integer (nullable = true)\n",
      " |-- days_since_prior_order: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# File path to the input CSV file\n",
    "csv_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/orders/\"\n",
    "\n",
    "# Read the CSV file with schema inference\n",
    "orders_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# \"overwrite\" ensures that any existing data at the specified output Parquet file path \n",
    "# is overwritten with the new data from the DataFrame\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/orders/orders.parquet\"\n",
    "orders_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "orders_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(parquet_file_path)\n",
    "\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- aisle_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File path to the input CSV file\n",
    "csv_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/products/\"\n",
    "\n",
    "# Read the CSV file with schema inference\n",
    "products_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# \"overwrite\" ensures that any existing data at the specified output Parquet file path \n",
    "# is overwritten with the new data from the DataFrame\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/products/products.parquet\"\n",
    "products_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "products_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(parquet_file_path)\n",
    "\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**order_products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 00:49:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 18:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# File path to the input CSV file\n",
    "csv_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/order_products/\"\n",
    "\n",
    "# Read the CSV file with schema inference\n",
    "order_products_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# \"overwrite\" ensures that any existing data at the specified output Parquet file path \n",
    "# is overwritten with the new data from the DataFrame\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/order_products/order_products.parquet\"\n",
    "order_products_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "order_products_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(parquet_file_path)\n",
    "\n",
    "order_products_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Spark transformations of assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import avg, sum, min, max, round, count, when, col, countDistinct, desc, asc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 00:49:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/06/10 00:50:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 24:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation job finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q1. Create a table called order_products_prior by using the last SQL query you created from the\n",
    "previous assignment. It should be similar to below (note you need to replace the s3 bucket\n",
    "name “imba” to yours own bucket name):\n",
    "\n",
    "    CREATE TABLE order_products_prior AS\n",
    "        (SELECT a.\n",
    "        *\n",
    "        ,\n",
    "        b.product_id,\n",
    "        b.add_to_cart_order,\n",
    "        b.reordered\n",
    "        FROM orders a\n",
    "        JOIN order_products b\n",
    "        ON a.order_id = b.order_id\n",
    "        WHERE a.eval_set = 'prior')\n",
    "\"\"\"\n",
    "\n",
    "order_products_prior_df = orders_df.filter(\n",
    "                          orders_df.eval_set == 'prior').join(order_products_df, \\\n",
    "                          orders_df.order_id == order_products_df.order_id, 'inner').select(\n",
    "                          orders_df[\"*\"], order_products_df.product_id, order_products_df.add_to_cart_order, order_products_df.reordered)\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/order_products_prior/order_products_prior.parquet\"\n",
    "order_products_prior_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "order_products_prior_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- eval_set: string (nullable = true)\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- order_dow: integer (nullable = true)\n",
      " |-- order_hour_of_day: integer (nullable = true)\n",
      " |-- days_since_prior_order: double (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+----------+-----------------+---------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|product_id|add_to_cart_order|reordered|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+----------+-----------------+---------+\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     30597|                1|        1|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     15221|                2|        1|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     43772|                3|        1|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     37886|                4|        1|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     37215|                5|        0|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     34335|                6|        1|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|      3164|                7|        0|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     26910|                8|        0|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     38888|                9|        0|\n",
      "|      12| 152610|   prior|          22|        6|                8|                  10.0|     38050|               10|        1|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_products_prior_df.printSchema()\n",
    "order_products_prior_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation job finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2.Create a SQL query (user_features_1). Based on table orders, for each user, calculate the\n",
    "max order_number, the sum of days_since_prior_order and the average of\n",
    "days_since_prior_order.\n",
    "\n",
    "SELECT user_id,\n",
    "       MAX(order_number) as max_order_number, \n",
    "       CAST(SUM(days_since_prior_order) AS INT) as sum_days_prior,\n",
    "       ROUND(AVG(days_since_prior_order),2) as avg_days_prior\n",
    "FROM orders \n",
    "GROUP BY user_id \n",
    "ORDER BY user_id;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_features_1_df = orders_df.withColumn(\"days_since_prior_order\", orders_df[\"days_since_prior_order\"].cast(IntegerType()) ). \\\n",
    "    groupBy(\"user_id\").agg(\n",
    "    max(\"order_number\").alias(\"max_order_number\"),\n",
    "    sum(\"days_since_prior_order\").alias(\"sum_days_prior\"),\n",
    "    round(avg(\"days_since_prior_order\"),).alias(\"avg_days_prior\") )\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/user_features_1/user_features_1.parquet\"\n",
    "user_features_1_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "user_features_1_df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------+--------------+\n",
      "|user_id|max_order_number|sum_days_prior|avg_days_prior|\n",
      "+-------+----------------+--------------+--------------+\n",
      "|      1|              11|           190|          19.0|\n",
      "|      2|              15|           228|          16.0|\n",
      "|      3|              13|           144|          12.0|\n",
      "|      4|               6|            85|          17.0|\n",
      "|      5|               5|            46|          12.0|\n",
      "|      6|               4|            40|          13.0|\n",
      "|      7|              21|           209|          10.0|\n",
      "|      8|               4|            70|          23.0|\n",
      "|      9|               4|            66|          22.0|\n",
      "|     10|               6|           109|          22.0|\n",
      "+-------+----------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_features_1_df = user_features_1_df.orderBy(user_features_1_df.user_id.asc())\n",
    "user_features_1_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3.Create a SQL query (user_features_2). Similar to above, based on table\n",
    "order_products_prior, for each user calculate the total number of products, total number of\n",
    "distinct products, and user reorder ratio(number of reordered = 1 divided by number of\n",
    "order_number > 1)\n",
    "\n",
    "\n",
    "WITH user_ratio AS (SELECT user_id, \n",
    "\t\t\t   COUNT(*) as product_bought, \n",
    "               COUNT(DISTINCT(product_id)) as unique_product_bought, \n",
    "\t\t\t   COUNT(CASE WHEN reordered = 1 THEN 1 ELSE NULL END) as num_reordered, \n",
    "               COUNT(CASE WHEN order_number > 1 THEN 1 ELSE NULL END) as num_order_number\n",
    "\t\t    FROM order_products_prior\n",
    "\t\t    GROUP BY user_id\n",
    "\t\t    ORDER BY user_id) SELECT user_id, \n",
    " \t\t\t\t\t     product_bought, \n",
    "\t\t\t\t\t     unique_product_bought, \n",
    "\t\t\t\t\t     num_reordered, num_order_number, \n",
    "\t\t\t\t\t     ROUND(CAST(num_reordered AS DOUBLE) / num_order_number ,4) AS reorder_ratio \n",
    "\t\t\t\t      FROM user_ratio\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_features_2_df = order_products_prior_df.groupBy(\"user_id\").agg(\n",
    "    count(\"*\").alias(\"num_product_bought\"),\n",
    "    countDistinct(\"product_id\").alias(\"num_distinct_product_bought\"),\n",
    "    round(count(when(col(\"reordered\") == 1, True)) / count(when(col(\"order_number\") > 1, True)),4).alias(\"reordered_ratio\")\n",
    ")\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/user_features_2/user_features_2.parquet\"\n",
    "user_features_2_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "user_features_2_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------------------------+---------------+\n",
      "|user_id|num_product_bought|num_distinct_product_bought|reordered_ratio|\n",
      "+-------+------------------+---------------------------+---------------+\n",
      "|      1|                59|                         18|         0.7593|\n",
      "|      2|               195|                        102|          0.511|\n",
      "|      3|                88|                         33|         0.7051|\n",
      "|      4|                18|                         17|         0.0714|\n",
      "|      5|                37|                         23|         0.5385|\n",
      "|      6|                14|                         12|            0.2|\n",
      "|      7|               206|                         68|         0.7113|\n",
      "|      8|                49|                         36|         0.4643|\n",
      "|      9|                76|                         58|         0.3913|\n",
      "|     10|               143|                         94|         0.3551|\n",
      "+-------+------------------+---------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_features_2_df.orderBy(user_features_2_df.user_id.asc()).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4:\n",
    "    Create a SQL query (up_features). Based on table order_products_prior, for each user and\n",
    "    product, calculate the total number of orders, minimum order_number, maximum\n",
    "    order_number and average add_to_cart_order.\n",
    "\n",
    "    SELECT user_id, \n",
    "       product_id, \n",
    "       COUNT(*) as num_of_orders, \n",
    "       MIN(order_number) as min_order_num, \n",
    "       MAX(order_number) as max_order_num, \n",
    "       ROUND(AVG(add_to_cart_order),2) as seq_add_to_order\n",
    "    FROM order_products_prior\n",
    "    GROUP BY user_id, product_id\n",
    "    ORDER BY user_id, product_id;\n",
    "\"\"\"\n",
    "\n",
    "up_features_df = order_products_prior_df.groupBy(\"user_id\",\"product_id\").agg(\n",
    "    count(\"*\").alias(\"number_of_orders\"),\n",
    "    min(\"order_number\").alias(\"min_order_num\"),\n",
    "    max(\"order_number\").alias(\"max_order_num\"),\n",
    "    round(avg(\"add_to_cart_order\"),2).alias(\"seq_add_to_order\")\n",
    ")\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/up_features/up_features.parquet\"\n",
    "up_features_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "up_features_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+-------------+-------------+----------------+\n",
      "|user_id|product_id|number_of_orders|min_order_num|max_order_num|seq_add_to_order|\n",
      "+-------+----------+----------------+-------------+-------------+----------------+\n",
      "|      1|       196|              10|            1|           10|             1.4|\n",
      "|      1|     10258|               9|            2|           10|            3.33|\n",
      "|      1|     17122|               1|            5|            5|             6.0|\n",
      "|      2|      2002|               4|            8|           11|           10.25|\n",
      "|      2|      4957|               1|            6|            6|            17.0|\n",
      "|      2|      7781|               3|            3|           14|             7.0|\n",
      "|      2|      9124|               1|            9|            9|            23.0|\n",
      "|      2|     12000|               5|            1|           10|             2.6|\n",
      "|      2|     13351|               2|            9|           12|            10.0|\n",
      "|      2|     14306|               1|            8|            8|            16.0|\n",
      "+-------+----------+----------------+-------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "up_features_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5. Create a SQL query (prd_features). Based on table order_products_prior, first write a sql\n",
    "query to calculate the sequence of product purchase for each user, and name it\n",
    "product_seq_time. Then on top of this query, for each product, calculate the count, sum of reordered, count of\n",
    "product_seq_time = 1 and count of product_seq_time = 2.\n",
    "\n",
    "WITH product_seq AS (SELECT user_id, \n",
    "\t\t\t    order_number, \n",
    "\t\t\t    product_id,\n",
    "\t\t\t    ROW_NUMBER() OVER (PARTITION BY user_id, product_id ORDER BY order_number ASC) AS product_seq_time,\n",
    "\t\t\t    reordered\n",
    "\t\t     FROM order_products_prior\n",
    "\t\t     ORDER BY user_id, order_number, product_seq_time) SELECT product_id, \n",
    "\t\t\t\t\t\t\t\t\t      COUNT(*) AS num_product_ordered, \n",
    "\t\t\t\t\t\t\t\t\t      SUM(reordered) as sum_reordered, \n",
    "\t\t\t\t\t\t\t\t\t      COUNT(CASE WHEN product_seq_time = 1 THEN 1 ELSE NULL END) as seq_is_one, \n",
    "\t\t\t\t\t\t\t\t\t      COUNT(CASE WHEN product_seq_time = 2 THEN 1 ELSE NULL END) as seq_is_two\n",
    "\t\t\t\t\t\t\t\t\t      FROM product_seq\n",
    "\t\t\t\t\t\t\t\t\t      GROUP BY product_id\n",
    "\t\t\t\t\t\t\t\t\t      ORDER BY product_id;\n",
    "\"\"\"\n",
    "\n",
    "# Define a Window specification to partition and order the data\n",
    "windowSpec = Window.partitionBy(\"user_id\", \"product_id\").orderBy(\"order_number\")\n",
    "prd_features_df = order_products_prior_df.withColumn(\"product_seq_time\", row_number().over(windowSpec))\n",
    "\n",
    "prd_features_df = prd_features_df.groupBy(\"product_id\").agg (\n",
    "                count(\"*\").alias(\"num_product_ordered\"),\n",
    "                sum(\"reordered\").alias(\"sum_reordered\"),\n",
    "                count(when(col(\"product_seq_time\") == 1, True)).alias(\"seq_is_one\"),\n",
    "                count(when(col(\"product_seq_time\") == 2, True)).alias(\"seq_is_two\")\n",
    ")\n",
    "\n",
    "parquet_file_path = \"/Users/tgou1055/jr_data/jr_project/imba_data/prd_features/prd_features.parquet\"\n",
    "prd_features_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "prd_features_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 124:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+----------+----------+\n",
      "|product_id|num_product_ordered|sum_reordered|seq_is_one|seq_is_two|\n",
      "+----------+-------------------+-------------+----------+----------+\n",
      "|         1|               1852|         1136|       716|       276|\n",
      "|         2|                 90|           12|        78|         8|\n",
      "|         3|                277|          203|        74|        36|\n",
      "|         4|                329|          147|       182|        64|\n",
      "|         5|                 15|            9|         6|         4|\n",
      "|         6|                  8|            3|         5|         2|\n",
      "|         7|                 30|           12|        18|         6|\n",
      "|         8|                165|           83|        82|        30|\n",
      "|         9|                156|           82|        74|        31|\n",
      "|        10|               2572|         1304|      1268|       399|\n",
      "+----------+-------------------+-------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prd_features.sort(asc(\"product_id\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
