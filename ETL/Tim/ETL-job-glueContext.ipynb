{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# AWS Glue Studio Notebook\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "####  Run this cell to set up and start your interactive session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%timeout 15\n",
    "%idle_timeout 15\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new data base imba_parquet ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Initialize the Glue client\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Create a new database\n",
    "response = glue_client.create_database(\n",
    "    DatabaseInput={\n",
    "        'Name': 'imba_parquet',\n",
    "        'Description': 'a new database that points to the parquet files',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage GlueContext to convert newly uploaded csv data files to parquet.\n",
    "\n",
    "**(need to consider the choices for updateBehavior, partitionKeys, enableUpdateCatalog)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aisles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata from database='imba'\n",
    "aisles_read = glueContext.create_dynamic_frame. \\\n",
    "    from_catalog(database=\"imba2\", table_name=\"aisles\", transformation_ctx=\"S3_out\")\n",
    "\n",
    "# write csv file to parquet file\n",
    "aisles_parquet = glueContext. \\\n",
    "getSink(path=\"s3://imba-tgou1055/data3/aisles/\", connection_type=\"s3\", \\\n",
    "        updateBehavior=\"LOG\", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx=\"S3_in\")\n",
    "aisles_parquet.setCatalogInfo(catalogDatabase=\"imba_parquet\",catalogTableName=\"aisles\")\n",
    "aisles_parquet.setFormat(\"glueparquet\", compression=\"snappy\")\n",
    "aisles_parquet.writeFrame(aisles_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**departments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata from database='imba'\n",
    "departments_read = glueContext.create_dynamic_frame. \\\n",
    "    from_catalog(database=\"imba2\", table_name=\"departments\", transformation_ctx=\"S3_out\")\n",
    "\n",
    "# write csv file to parquet file\n",
    "departments_parquet = glueContext. \\\n",
    "getSink(path=\"s3://imba-tgou1055/data3/departments/\", connection_type=\"s3\", \\\n",
    "        updateBehavior=\"LOG\", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx=\"S3_in\")\n",
    "departments_parquet.setCatalogInfo(catalogDatabase=\"imba_parquet\",catalogTableName=\"departments\")\n",
    "departments_parquet.setFormat(\"glueparquet\", compression=\"snappy\")\n",
    "departments_parquet.writeFrame(departments_read)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata from database='imba'\n",
    "orders_read = glueContext.create_dynamic_frame. \\\n",
    "    from_catalog(database=\"imba2\", table_name=\"orders\", transformation_ctx=\"S3_out\")\n",
    "\n",
    "# write csv file to parquet file\n",
    "orders_parquet = glueContext. \\\n",
    "getSink(path=\"s3://imba-tgou1055/data3/orders/\", connection_type=\"s3\", \\\n",
    "        updateBehavior=\"LOG\", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx=\"S3_in\")\n",
    "orders_parquet.setCatalogInfo(catalogDatabase=\"imba_parquet\",catalogTableName=\"orders\")\n",
    "orders_parquet.setFormat(\"glueparquet\", compression=\"snappy\")\n",
    "orders_parquet.writeFrame(orders_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata from database='imba'\n",
    "products_read = glueContext.create_dynamic_frame. \\\n",
    "    from_catalog(database=\"imba2\", table_name=\"products\", transformation_ctx=\"S3_out\")\n",
    "\n",
    "# write csv file to parquet file\n",
    "products_parquet = glueContext. \\\n",
    "getSink(path=\"s3://imba-tgou1055/data3/products/\", connection_type=\"s3\", \\\n",
    "        updateBehavior=\"LOG\", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx=\"S3_in\")\n",
    "products_parquet.setCatalogInfo(catalogDatabase=\"imba_parquet\",catalogTableName=\"products\")\n",
    "products_parquet.setFormat(\"glueparquet\", compression=\"snappy\")\n",
    "products_parquet.writeFrame(products_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**order_products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata from database='imba'\n",
    "order_products_read = glueContext.create_dynamic_frame. \\\n",
    "    from_catalog(database=\"imba2\", table_name=\"order_products\", transformation_ctx=\"S3_out\")\n",
    "\n",
    "# write csv file to parquet file\n",
    "order_products_parquet = glueContext. \\\n",
    "getSink(path=\"s3://imba-tgou1055/data3/order_products/\", connection_type=\"s3\", \\\n",
    "        updateBehavior=\"LOG\", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx=\"S3_in\")\n",
    "order_products_parquet.setCatalogInfo(catalogDatabase=\"imba_parquet\",catalogTableName=\"order_products\")\n",
    "order_products_parquet.setFormat(\"glueparquet\", compression=\"snappy\")\n",
    "order_products_parquet.writeFrame(order_products_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commit job\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Spark transformations of assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import avg, sum, min, max, round, count, when, col, countDistinct, desc, asc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. Create a table called order_products_prior by using the last SQL query you created from the\n",
    "previous assignment. It should be similar to below (note you need to replace the s3 bucket\n",
    "name “imba” to yours own bucket name):\n",
    "\n",
    "    CREATE TABLE order_products_prior AS\n",
    "        (SELECT a.\n",
    "        *\n",
    "        ,\n",
    "        b.product_id,\n",
    "        b.add_to_cart_order,\n",
    "        b.reordered\n",
    "        FROM orders a\n",
    "        JOIN order_products b\n",
    "        ON a.order_id = b.order_id\n",
    "        WHERE a.eval_set = 'prior')\n",
    "\"\"\"\n",
    "orders_df = spark.read.parquet(\"s3://imba-tgou1055/data3/orders/\")\n",
    "order_products_df = spark.read.parquet(\"s3://imba-tgou1055/data3/order_products/\")\n",
    "\n",
    "order_products_prior_df = orders_df.filter(\n",
    "                          orders_df.eval_set == 'prior').join(order_products_df, \\\n",
    "                          orders_df.order_id == order_products_df.order_id, 'inner').select(\n",
    "                          orders_df[\"*\"], order_products_df.product_id, order_products_df.add_to_cart_order, order_products_df.reordered)\n",
    "\n",
    "parquet_file_path = \"s3://imba-tgou1055/data3/order_products_prior/\"\n",
    "order_products_prior_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "\n",
    "# load parquet file\n",
    "order_products_prior_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2.Create a SQL query (user_features_1). Based on table orders, for each user, calculate the\n",
    "max order_number, the sum of days_since_prior_order and the average of\n",
    "days_since_prior_order.\n",
    "\n",
    "SELECT user_id,\n",
    "       MAX(order_number) as max_order_number, \n",
    "       CAST(SUM(days_since_prior_order) AS INT) as sum_days_prior,\n",
    "       ROUND(AVG(days_since_prior_order),2) as avg_days_prior\n",
    "FROM orders \n",
    "GROUP BY user_id \n",
    "ORDER BY user_id;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "orders_df = spark.read.parquet(\"s3://imba-tgou1055/data3/orders/\")\n",
    "\n",
    "user_features_1_df = orders_df.withColumn(\"days_since_prior_order\", orders_df[\"days_since_prior_order\"].cast(IntegerType()) ). \\\n",
    "    groupBy(\"user_id\").agg(\n",
    "    max(\"order_number\").alias(\"max_order_number\"),\n",
    "    sum(\"days_since_prior_order\").alias(\"sum_days_prior\"),\n",
    "    round(avg(\"days_since_prior_order\"),).alias(\"avg_days_prior\") )\n",
    "\n",
    "parquet_file_path = \"s3://imba-tgou1055/data3/user_features_1/\"\n",
    "user_features_1_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "#user_features_1_df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3.Create a SQL query (user_features_2). Similar to above, based on table\n",
    "order_products_prior, for each user calculate the total number of products, total number of\n",
    "distinct products, and user reorder ratio(number of reordered = 1 divided by number of\n",
    "order_number > 1)\n",
    "\n",
    "\n",
    "WITH user_ratio AS (SELECT user_id, \n",
    "\t\t\t   COUNT(*) as product_bought, \n",
    "               COUNT(DISTINCT(product_id)) as unique_product_bought, \n",
    "\t\t\t   COUNT(CASE WHEN reordered = 1 THEN 1 ELSE NULL END) as num_reordered, \n",
    "               COUNT(CASE WHEN order_number > 1 THEN 1 ELSE NULL END) as num_order_number\n",
    "\t\t    FROM order_products_prior\n",
    "\t\t    GROUP BY user_id\n",
    "\t\t    ORDER BY user_id) SELECT user_id, \n",
    " \t\t\t\t\t     product_bought, \n",
    "\t\t\t\t\t     unique_product_bought, \n",
    "\t\t\t\t\t     num_reordered, num_order_number, \n",
    "\t\t\t\t\t     ROUND(CAST(num_reordered AS DOUBLE) / num_order_number ,4) AS reorder_ratio \n",
    "\t\t\t\t      FROM user_ratio\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_features_2_df = order_products_prior_df.groupBy(\"user_id\").agg(\n",
    "    count(\"*\").alias(\"num_product_bought\"),\n",
    "    countDistinct(\"product_id\").alias(\"num_distinct_product_bought\"),\n",
    "    round(count(when(col(\"reordered\") == 1, True)) / count(when(col(\"order_number\") > 1, True)),4).alias(\"reordered_ratio\")\n",
    ")\n",
    "\n",
    "parquet_file_path = \"s3://imba-tgou1055/data3/user_features_2/\"\n",
    "user_features_2_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "user_features_2_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4:\n",
    "    Create a SQL query (up_features). Based on table order_products_prior, for each user and\n",
    "    product, calculate the total number of orders, minimum order_number, maximum\n",
    "    order_number and average add_to_cart_order.\n",
    "\n",
    "    SELECT user_id, \n",
    "       product_id, \n",
    "       COUNT(*) as num_of_orders, \n",
    "       MIN(order_number) as min_order_num, \n",
    "       MAX(order_number) as max_order_num, \n",
    "       ROUND(AVG(add_to_cart_order),2) as seq_add_to_order\n",
    "    FROM order_products_prior\n",
    "    GROUP BY user_id, product_id\n",
    "    ORDER BY user_id, product_id;\n",
    "\"\"\"\n",
    "\n",
    "up_features_df = order_products_prior_df.groupBy(\"user_id\",\"product_id\").agg(\n",
    "    count(\"*\").alias(\"number_of_orders\"),\n",
    "    min(\"order_number\").alias(\"min_order_num\"),\n",
    "    max(\"order_number\").alias(\"max_order_num\"),\n",
    "    round(avg(\"add_to_cart_order\"),2).alias(\"seq_add_to_order\")\n",
    ")\n",
    "\n",
    "parquet_file_path = \"s3://imba-tgou1055/data3/up_features/\"\n",
    "up_features_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "up_features_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5. Create a SQL query (prd_features). Based on table order_products_prior, first write a sql\n",
    "query to calculate the sequence of product purchase for each user, and name it\n",
    "product_seq_time. Then on top of this query, for each product, calculate the count, sum of reordered, count of\n",
    "product_seq_time = 1 and count of product_seq_time = 2.\n",
    "\n",
    "WITH product_seq AS (SELECT user_id, \n",
    "\t\t\t    order_number, \n",
    "\t\t\t    product_id,\n",
    "\t\t\t    ROW_NUMBER() OVER (PARTITION BY user_id, product_id ORDER BY order_number ASC) AS product_seq_time,\n",
    "\t\t\t    reordered\n",
    "\t\t     FROM order_products_prior\n",
    "\t\t     ORDER BY user_id, order_number, product_seq_time) SELECT product_id, \n",
    "\t\t\t\t\t\t\t\t\t      COUNT(*) AS num_product_ordered, \n",
    "\t\t\t\t\t\t\t\t\t      SUM(reordered) as sum_reordered, \n",
    "\t\t\t\t\t\t\t\t\t      COUNT(CASE WHEN product_seq_time = 1 THEN 1 ELSE NULL END) as seq_is_one, \n",
    "\t\t\t\t\t\t\t\t\t      COUNT(CASE WHEN product_seq_time = 2 THEN 1 ELSE NULL END) as seq_is_two\n",
    "\t\t\t\t\t\t\t\t\t      FROM product_seq\n",
    "\t\t\t\t\t\t\t\t\t      GROUP BY product_id\n",
    "\t\t\t\t\t\t\t\t\t      ORDER BY product_id;\n",
    "\"\"\"\n",
    "\n",
    "# Define a Window specification to partition and order the data\n",
    "windowSpec = Window.partitionBy(\"user_id\", \"product_id\").orderBy(\"order_number\")\n",
    "prd_features_df = order_products_prior_df.withColumn(\"product_seq_time\", row_number().over(windowSpec))\n",
    "\n",
    "prd_features_df = prd_features_df.groupBy(\"product_id\").agg (\n",
    "                count(\"*\").alias(\"num_product_ordered\"),\n",
    "                sum(\"reordered\").alias(\"sum_reordered\"),\n",
    "                count(when(col(\"product_seq_time\") == 1, True)).alias(\"seq_is_one\"),\n",
    "                count(when(col(\"product_seq_time\") == 2, True)).alias(\"seq_is_two\")\n",
    ")\n",
    "\n",
    "parquet_file_path = \"s3://imba-tgou1055/data3/prd_features/\"\n",
    "prd_features_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
    "prd_features_df = spark.read.parquet(parquet_file_path)\n",
    "print(\"transformation job finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop sparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
