{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.5 \n"
					]
				}
			],
			"source": [
				"# %help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Current idle_timeout is 20 minutes.\n",
						"idle_timeout has been set to 20 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: G.1X\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: 2\n",
						"Setting new number of workers to: 2\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 2\n",
						"Idle Timeout: 20\n",
						"Session ID: f5c32082-b131-425e-a62a-0b638e6cce43\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.5\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session f5c32082-b131-425e-a62a-0b638e6cce43 to get into ready status...\n",
						"Session f5c32082-b131-425e-a62a-0b638e6cce43 has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"%idle_timeout 20\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# important! using python min, max won't work\n",
				"from pyspark.sql.functions import col, min, max, sum, avg, count, countDistinct, row_number\n",
				"from pyspark.sql.window import Window\n",
				"\n",
				"# https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
				"from pyspark.sql.types import StructType, StructField, BooleanType, ByteType, ShortType, IntegerType, StringType, FloatType, DoubleType"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## aisles\n",
				"read as csv, save as parquet, then read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Py4JJavaError: An error occurred while calling o138.parquet.\n",
						": java.io.IOException: Failed to delete key: aisles\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:417)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.delete(EmrFileSystem.java:387)\n",
						"\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:186)\n",
						"\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.deleteWithJob(SQLEmrOptimizedCommitProtocol.scala:149)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:257)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:133)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
						"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
						"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.lang.Thread.run(Thread.java:750)\n",
						"Caused by: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: WYDHAP94SWK9KQM6; S3 Extended Request ID: H+3yDCPtjU1BvFkULqGB8qe7iPQgwX0H/skr2g66ikmS62nOUp6OrlfL7+FEaruFXyLq9lHOZgdXuBly8VPZPw==; Proxy: null), S3 Extended Request ID: H+3yDCPtjU1BvFkULqGB8qe7iPQgwX0H/skr2g66ikmS62nOUp6OrlfL7+FEaruFXyLq9lHOZgdXuBly8VPZPw==\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.handleAmazonServiceException(Jets3tNativeFileSystemStore.java:618)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:442)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.deleteAll(Jets3tNativeFileSystemStore.java:493)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.doSingleThreadedBatchDelete(S3NativeFileSystem.java:1104)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:415)\n",
						"\t... 51 more\n",
						"Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: WYDHAP94SWK9KQM6; S3 Extended Request ID: H+3yDCPtjU1BvFkULqGB8qe7iPQgwX0H/skr2g66ikmS62nOUp6OrlfL7+FEaruFXyLq9lHOZgdXuBly8VPZPw==; Proxy: null), S3 Extended Request ID: H+3yDCPtjU1BvFkULqGB8qe7iPQgwX0H/skr2g66ikmS62nOUp6OrlfL7+FEaruFXyLq9lHOZgdXuBly8VPZPw==\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.deleteObject(AmazonS3Client.java:2302)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:25)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:11)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.deleteObject(AmazonS3LiteClient.java:122)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:436)\n",
						"\t... 54 more\n",
						"\n"
					]
				}
			],
			"source": [
				"aisles_schema = StructType([\n",
				"    StructField(\"aisle_id\", IntegerType(), True),\n",
				"    StructField(\"aisle\", StringType(), True)\n",
				"])\n",
				"aisles = spark.read.csv(\"s3://imba-derek/data/aisles/\", header=True, schema=aisles_schema)\n",
				"aisles.write.mode(\"overwrite\").parquet(\"s3://derek-raw-parquet/aisles/\")\n",
				"aisles = spark.read.parquet('s3://derek-raw-parquet/aisles/')\n",
				"aisles.printSchema()\n",
				"print(f'row count: {aisles.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## departments\n",
				"read as csv, save as parquet, then read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Py4JJavaError: An error occurred while calling o155.parquet.\n",
						": java.io.IOException: Failed to delete key: departments\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:417)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.delete(EmrFileSystem.java:387)\n",
						"\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:186)\n",
						"\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.deleteWithJob(SQLEmrOptimizedCommitProtocol.scala:149)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:257)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:133)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
						"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
						"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.lang.Thread.run(Thread.java:750)\n",
						"Caused by: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: ES18KTDMN78AMBJS; S3 Extended Request ID: FxVhPVZNbU39Gdj4DvpAdyOpyZvWWQPcDZ1kxJHHlZ3EmspvFm3DscWuKt3K6baVRocnxKyQ7mA=; Proxy: null), S3 Extended Request ID: FxVhPVZNbU39Gdj4DvpAdyOpyZvWWQPcDZ1kxJHHlZ3EmspvFm3DscWuKt3K6baVRocnxKyQ7mA=\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.handleAmazonServiceException(Jets3tNativeFileSystemStore.java:618)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:442)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.deleteAll(Jets3tNativeFileSystemStore.java:493)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.doSingleThreadedBatchDelete(S3NativeFileSystem.java:1104)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:415)\n",
						"\t... 51 more\n",
						"Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: ES18KTDMN78AMBJS; S3 Extended Request ID: FxVhPVZNbU39Gdj4DvpAdyOpyZvWWQPcDZ1kxJHHlZ3EmspvFm3DscWuKt3K6baVRocnxKyQ7mA=; Proxy: null), S3 Extended Request ID: FxVhPVZNbU39Gdj4DvpAdyOpyZvWWQPcDZ1kxJHHlZ3EmspvFm3DscWuKt3K6baVRocnxKyQ7mA=\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.deleteObject(AmazonS3Client.java:2302)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:25)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:11)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.deleteObject(AmazonS3LiteClient.java:122)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:436)\n",
						"\t... 54 more\n",
						"\n"
					]
				}
			],
			"source": [
				"departments_schema = StructType([\n",
				"    StructField(\"department_id\", IntegerType(), True),\n",
				"    StructField(\"department\", StringType(), True)\n",
				"])\n",
				"departments = spark.read.csv(\"s3://imba-derek/data/departments/departments.csv\", header=True, schema=departments_schema)\n",
				"departments.write.mode(\"overwrite\").parquet(\"s3://derek-raw-parquet/departments/\")\n",
				"departments = spark.read.parquet('s3://derek-raw-parquet/departments') # read as parquet\n",
				"departments.printSchema()\n",
				"print(f'row count: {departments.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## products\n",
				"read as csv, save as parquet, then read from parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Py4JJavaError: An error occurred while calling o172.parquet.\n",
						": java.io.IOException: Failed to delete key: products\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:417)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.delete(EmrFileSystem.java:387)\n",
						"\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:186)\n",
						"\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.deleteWithJob(SQLEmrOptimizedCommitProtocol.scala:149)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:257)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:133)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
						"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
						"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.lang.Thread.run(Thread.java:750)\n",
						"Caused by: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9BWMG6Y4QNGB5GMB; S3 Extended Request ID: MgdmvxV3HMB4bBJRYmzyUqSr1aoFbgoJXqvU2g0+oLsDYjQPOpXTynlf+SdeHMcZEIXGk7M5OM846/BhzZ7uoQ==; Proxy: null), S3 Extended Request ID: MgdmvxV3HMB4bBJRYmzyUqSr1aoFbgoJXqvU2g0+oLsDYjQPOpXTynlf+SdeHMcZEIXGk7M5OM846/BhzZ7uoQ==\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.handleAmazonServiceException(Jets3tNativeFileSystemStore.java:618)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:442)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.deleteAll(Jets3tNativeFileSystemStore.java:493)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.doSingleThreadedBatchDelete(S3NativeFileSystem.java:1104)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:415)\n",
						"\t... 51 more\n",
						"Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9BWMG6Y4QNGB5GMB; S3 Extended Request ID: MgdmvxV3HMB4bBJRYmzyUqSr1aoFbgoJXqvU2g0+oLsDYjQPOpXTynlf+SdeHMcZEIXGk7M5OM846/BhzZ7uoQ==; Proxy: null), S3 Extended Request ID: MgdmvxV3HMB4bBJRYmzyUqSr1aoFbgoJXqvU2g0+oLsDYjQPOpXTynlf+SdeHMcZEIXGk7M5OM846/BhzZ7uoQ==\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.deleteObject(AmazonS3Client.java:2302)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:25)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:11)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.deleteObject(AmazonS3LiteClient.java:122)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:436)\n",
						"\t... 54 more\n",
						"\n"
					]
				}
			],
			"source": [
				"products_schema = StructType([\n",
				"    StructField(\"product_id\", IntegerType(), True),\n",
				"    StructField(\"product_name\", StringType(), True),\n",
				"    StructField(\"aisle_id\", IntegerType(), True),\n",
				"    StructField(\"department_id\", IntegerType(), True)\n",
				"])\n",
				"products = spark.read.csv(\"s3://imba-derek/data/products/products.csv\", header=True, schema=products_schema)\n",
				"products.write.mode(\"overwrite\").parquet(\"s3://derek-raw-parquet/products/\")\n",
				"products = spark.read.parquet('s3://derek-raw-parquet/products') # read as parquet\n",
				"products.printSchema()\n",
				"print(f'row count: {products.count()}')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"## denorm products\n",
				"join with aisles and departments, save to transformed"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						"\n",
						" |-- product_id: integer (nullable = true)\n",
						"\n",
						" |-- product_name: string (nullable = true)\n",
						"\n",
						" |-- aisle_id: integer (nullable = true)\n",
						"\n",
						" |-- aisle: string (nullable = true)\n",
						"\n",
						" |-- department_id: integer (nullable = true)\n",
						"\n",
						" |-- department: string (nullable = true)\n"
					]
				}
			],
			"source": [
				"products_denorm = products\\\n",
				"                    .join(aisles, products.aisle_id==aisles.aisle_id, 'inner')\\\n",
				"                    .join(departments, products.department_id==departments.department_id, 'inner')\\\n",
				"                    .select(products.product_id,\n",
				"                            products.product_name,\n",
				"                            products.aisle_id,\n",
				"                            aisles.aisle,\n",
				"                            products.department_id,\n",
				"                            departments.department\n",
				"                           )\n",
				"products_denorm.printSchema()\n",
				"products_denorm.write.mode(\"overwrite\").parquet(\"s3://derek-transformed-data/products/\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## orders\n",
				"read as csv, partition by eval_set, save as parquet, then read from parque"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Py4JJavaError: An error occurred while calling o192.parquet.\n",
						": java.io.IOException: Failed to delete key: orders\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:417)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.delete(EmrFileSystem.java:387)\n",
						"\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:186)\n",
						"\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.deleteWithJob(SQLEmrOptimizedCommitProtocol.scala:149)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:257)\n",
						"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:133)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
						"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
						"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
						"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
						"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
						"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
						"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.lang.Thread.run(Thread.java:750)\n",
						"Caused by: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: J9K4VM4ABWWVBYWV; S3 Extended Request ID: uG9H2ddTqlZJ2y9NNNwSeBY6KxC9hVVmUK7GDzygE7MUPyo+xRe5OdLvve/qLfiQB5Xco1Kyp9c=; Proxy: null), S3 Extended Request ID: uG9H2ddTqlZJ2y9NNNwSeBY6KxC9hVVmUK7GDzygE7MUPyo+xRe5OdLvve/qLfiQB5Xco1Kyp9c=\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.handleAmazonServiceException(Jets3tNativeFileSystemStore.java:618)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:442)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.deleteAll(Jets3tNativeFileSystemStore.java:493)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.doSingleThreadedBatchDelete(S3NativeFileSystem.java:1104)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.delete(S3NativeFileSystem.java:415)\n",
						"\t... 51 more\n",
						"Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: J9K4VM4ABWWVBYWV; S3 Extended Request ID: uG9H2ddTqlZJ2y9NNNwSeBY6KxC9hVVmUK7GDzygE7MUPyo+xRe5OdLvve/qLfiQB5Xco1Kyp9c=; Proxy: null), S3 Extended Request ID: uG9H2ddTqlZJ2y9NNNwSeBY6KxC9hVVmUK7GDzygE7MUPyo+xRe5OdLvve/qLfiQB5Xco1Kyp9c=\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.deleteObject(AmazonS3Client.java:2302)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:25)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.DeleteObjectCall.perform(DeleteObjectCall.java:11)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.deleteObject(AmazonS3LiteClient.java:122)\n",
						"\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:436)\n",
						"\t... 54 more\n",
						"\n"
					]
				}
			],
			"source": [
				"orders_schema = StructType([\n",
				"    StructField(\"order_id\", IntegerType(), True),\n",
				"    StructField(\"user_id\", IntegerType(), True),\n",
				"    StructField(\"eval_set\", StringType(), True),\n",
				"    StructField(\"order_number\", IntegerType(), True),\n",
				"    StructField(\"order_dow\", ByteType(), True),\n",
				"    StructField(\"order_hour_of_day\", ByteType(), True),\n",
				"    StructField(\"days_since_prior_order\", FloatType(), True)\n",
				"])\n",
				"orders = spark.read.csv(\"s3://imba-derek/data/orders/orders.csv\", header=True, schema=orders_schema)\n",
				"orders.write.partitionBy(\"eval_set\").mode(\"overwrite\").parquet(\"s3://derek-raw-parquet/orders/\")\n",
				"orders = spark.read.parquet('s3://derek-raw-parquet/orders') # read as parquet\n",
				"orders.printSchema()\n",
				"print(f'row count: {orders.count()}')\n",
				"orders.agg(min('order_number'), max('order_number')).show()\n",
				"orders.agg(min('days_since_prior_order'), max('days_since_prior_order')).show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"row count: 3214874\n"
					]
				}
			],
			"source": [
				"# filter by eval_set=prior\n",
				"orders_prior = orders.where(orders.eval_set=='prior').select(*[c for c in orders.columns if c!='eval_set'])\n",
				"print(f'row count: {orders_prior.count()}')\n",
				"orders_prior.write.mode(\"overwrite\").parquet(\"s3://derek-transformed-data/orders_prior/\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## order_products\n",
				"read as csv, save as parquet, then read from parque"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						"\n",
						" |-- order_id: integer (nullable = true)\n",
						"\n",
						" |-- product_id: integer (nullable = true)\n",
						"\n",
						" |-- add_to_cart_order: integer (nullable = true)\n",
						"\n",
						" |-- reordered: boolean (nullable = true)\n",
						"\n",
						"\n",
						"\n",
						"row count: 33819106\n"
					]
				}
			],
			"source": [
				"# takes 1 minute to run\n",
				"order_products_schema = StructType([\n",
				"    StructField(\"order_id\", IntegerType(), True),\n",
				"    StructField(\"product_id\", IntegerType(), True),\n",
				"    StructField(\"add_to_cart_order\", IntegerType(), True),\n",
				"    StructField(\"reordered\", IntegerType(), True)\n",
				"])\n",
				"order_products = spark.read.csv(\"s3://imba-derek/data/order_products/\", header=True, schema=order_products_schema)\n",
				"order_products = order_products.withColumn(\"reordered\", col(\"reordered\").cast(\"boolean\"))\n",
				"order_products.write.mode(\"overwrite\").parquet(\"s3://derek-raw-parquet/order_products/\")\n",
				"order_products = spark.read.parquet('s3://derek-raw-parquet/order_products') # read as parquet\n",
				"order_products.printSchema()\n",
				"print(f'row count: {order_products.count()}')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"NameError: name 'orders_prior' is not defined\n"
					]
				}
			],
			"source": [
				"# takes 20 seconds to run\n",
				"order_products_prior = orders_prior\\\n",
				"                        .join(order_products, orders_prior.order_id==order_products.order_id, 'inner')\\\n",
				"                        .select(orders_prior.order_id,\n",
				"                                orders_prior.user_id,\n",
				"                                orders_prior.order_number,\n",
				"                                orders_prior.order_dow,\n",
				"                                orders_prior.order_hour_of_day,\n",
				"                                orders_prior.days_since_prior_order,\n",
				"                                order_products.product_id,\n",
				"                                order_products.add_to_cart_order,\n",
				"                                order_products.reordered\n",
				"                               )\n",
				"order_products_prior.write.mode(\"overwrite\").parquet(\"s3://derek-transformed-data/order_products_prior/\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q2\n",
				"```sql\n",
				"select \n",
				"    user_id, \n",
				"    max(order_number) as max_order_number, \n",
				"    sum(days_since_prior_order) as sum_days_since_prior_order, \n",
				"    avg(days_since_prior_order) as avg_days_since_prior_order\n",
				"from orders\n",
				"group by user_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# orders = spark.read.parquet('s3://sam-raw-parquet/orders') # read as parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+-------+----------------+--------------------------+--------------------------+\n",
						"\n",
						"|user_id|max_order_number|sum_days_since_prior_order|avg_days_since_prior_order|\n",
						"\n",
						"+-------+----------------+--------------------------+--------------------------+\n",
						"\n",
						"|      1|              11|                     190.0|                      19.0|\n",
						"\n",
						"|      2|              15|                     228.0|        16.285714285714285|\n",
						"\n",
						"|      3|              13|                     144.0|                      12.0|\n",
						"\n",
						"|      4|               6|                      85.0|                      17.0|\n",
						"\n",
						"|      5|               5|                      46.0|                      11.5|\n",
						"\n",
						"+-------+----------------+--------------------------+--------------------------+\n",
						"\n",
						"only showing top 5 rows\n",
						"\n",
						"\n",
						"\n",
						"row count: 206209\n"
					]
				}
			],
			"source": [
				"user_features_1 = orders.groupBy('user_id').agg(max('order_number').alias('max_order_number'),\n",
				"                                               sum('days_since_prior_order').alias('sum_days_since_prior_order'),\n",
				"                                               avg('days_since_prior_order').alias('avg_days_since_prior_order')\n",
				"                                               )\n",
				"user_features_1.orderBy('user_id').show(5)\n",
				"print(f'row count: {user_features_1.count()}')\n",
				"# save aggregated result as one part\n",
				"user_features_1.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/user_features_1/\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q3\n",
				"```sql\n",
				"SELECT\n",
				"    user_id,\n",
				"    COUNT(product_id) AS total_products_count,\n",
				"    COUNT(DISTINCT product_id) AS total_distinct_products_count, \n",
				"    SUM(CASE WHEN reordered = 1 THEN 1 ELSE 0 END) * 1.0 / \n",
				"    SUM(CASE WHEN order_number > 1 THEN 1 ELSE 0 END) AS reorder_ratio\n",
				"FROM order_products_prior\n",
				"GROUP BY user_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# order_products_prior = spark.read.parquet('s3://sam-transformed/order_products_prior') # read as parquet"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+-------+--------------+-----------------------+-------------------+\n",
						"\n",
						"|user_id|total_products|total_distinct_products|      reorder_ratio|\n",
						"\n",
						"+-------+--------------+-----------------------+-------------------+\n",
						"\n",
						"|      1|            59|                     18| 0.7592592592592593|\n",
						"\n",
						"|      2|           195|                    102|  0.510989010989011|\n",
						"\n",
						"|      3|            88|                     33| 0.7051282051282052|\n",
						"\n",
						"|      4|            18|                     17|0.07142857142857142|\n",
						"\n",
						"|      5|            37|                     23| 0.5384615384615384|\n",
						"\n",
						"+-------+--------------+-----------------------+-------------------+\n",
						"\n",
						"only showing top 5 rows\n",
						"\n",
						"\n",
						"\n",
						"row count: 206209\n"
					]
				}
			],
			"source": [
				"user_features_2 = order_products_prior.groupBy('user_id').agg(count('product_id').alias('total_products'),\n",
				"                                                              countDistinct('product_id').alias('total_distinct_products'),\n",
				"                                                              (sum(col('reordered').cast('int'))/\n",
				"                                                               sum((col('order_number')>1).cast('int'))).alias('reorder_ratio')\n",
				"                                                            )\n",
				"user_features_2.orderBy('user_id').show(5)\n",
				"print(f'row count: {user_features_2.count()}')\n",
				"user_features_2.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/user_features_2/\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q4\n",
				"```sql\n",
				"SELECT\n",
				"    user_id,\n",
				"    product_id,\n",
				"    COUNT(order_id) AS total_orders,\n",
				"    MIN(order_number) AS min_order_number,\n",
				"    MAX(order_number) AS max_order_number,\n",
				"    AVG(add_to_cart_order) AS avg_add_to_cart_order\n",
				"FROM order_products_prior\n",
				"GROUP BY user_id, product_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+-------+----------+------------+----------------+----------------+---------------------+\n",
						"\n",
						"|user_id|product_id|total_orders|min_order_number|max_order_number|avg_add_to_cart_order|\n",
						"\n",
						"+-------+----------+------------+----------------+----------------+---------------------+\n",
						"\n",
						"|      1|       196|          10|               1|              10|                  1.4|\n",
						"\n",
						"|      1|     10258|           9|               2|              10|   3.3333333333333335|\n",
						"\n",
						"|      1|     10326|           1|               5|               5|                  5.0|\n",
						"\n",
						"|      1|     12427|          10|               1|              10|                  3.3|\n",
						"\n",
						"|      1|     13032|           3|               2|              10|    6.333333333333333|\n",
						"\n",
						"+-------+----------+------------+----------------+----------------+---------------------+\n",
						"\n",
						"only showing top 5 rows\n",
						"\n",
						"\n",
						"\n",
						"row count: 13307953\n"
					]
				}
			],
			"source": [
				"up_features = order_products_prior.groupBy('user_id', 'product_id').agg(count('order_id').alias('total_orders'),\n",
				"                                                                        min('order_number').alias('min_order_number'),\n",
				"                                                                        max('order_number').alias('max_order_number'),\n",
				"                                                                        avg('add_to_cart_order').alias('avg_add_to_cart_order')\n",
				"                                                                       )\n",
				"up_features.orderBy('user_id', 'product_id').show(5)\n",
				"print(f'row count: {up_features.count()}')\n",
				"up_features.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/up_features/\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q5\n",
				"```sql\n",
				"SELECT \n",
				"    product_id,\n",
				"    COUNT(product_id) AS total_products,\n",
				"    SUM(reordered) AS total_reordered,\n",
				"    SUM(CASE WHEN product_seq_time = 1 THEN 1 ELSE 0 END) AS product_seq_time_is_1,\n",
				"    SUM(CASE WHEN product_seq_time = 2 THEN 1 ELSE 0 END) AS product_seq_time_is_2\n",
				"FROM (\n",
				"    SELECT\n",
				"        product_id,\n",
				"        reordered,\n",
				"        ROW_NUMBER() OVER (PARTITION BY user_id, product_id ORDER BY order_number ASC) AS product_seq_time\n",
				"    FROM order_products_prior\n",
				") prod_seq\n",
				"GROUP BY product_id;\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----------+--------------+---------------+---------------------+---------------------+\n",
						"\n",
						"|product_id|total_products|total_reordered|product_seq_time_is_1|product_seq_time_is_2|\n",
						"\n",
						"+----------+--------------+---------------+---------------------+---------------------+\n",
						"\n",
						"|         1|          1852|           1136|                  716|                  276|\n",
						"\n",
						"|         2|            90|             12|                   78|                    8|\n",
						"\n",
						"|         3|           277|            203|                   74|                   36|\n",
						"\n",
						"|         4|           329|            147|                  182|                   64|\n",
						"\n",
						"|         5|            15|              9|                    6|                    4|\n",
						"\n",
						"+----------+--------------+---------------+---------------------+---------------------+\n",
						"\n",
						"only showing top 5 rows\n",
						"\n",
						"\n",
						"\n",
						"row count: 49677\n"
					]
				}
			],
			"source": [
				"prod_seq = order_products_prior.withColumn('product_seq_time', \n",
				"                                           row_number().over(Window\\\n",
				"                                                             .partitionBy('user_id', 'product_id')\\\n",
				"                                                             .orderBy(col('order_number').asc())\n",
				"                                                            )\n",
				"                                          ).select('product_id', 'reordered', 'product_seq_time')\n",
				"\n",
				"prd_features = prod_seq.groupBy('product_id').agg(count('product_id').alias('total_products'),\n",
				"                                                  sum(col('reordered').cast('int')).alias('total_reordered'),\n",
				"                                                  sum((col('product_seq_time')==1).cast('int')).alias('product_seq_time_is_1'),\n",
				"                                                  sum((col('product_seq_time')==2).cast('int')).alias('product_seq_time_is_2')\n",
				"                                                 )\n",
				"prd_features.orderBy('product_id').show(5)\n",
				"print(f'row count: {prd_features.count()}')\n",
				"prd_features.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/prd_features/\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
