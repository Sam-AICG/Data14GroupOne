{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# %help",
			"metadata": {
				"trusted": true,
				"editable": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": "Welcome to the Glue Interactive Sessions Kernel\n\n\n\nFor more information on available magic commands, please type %help in any new cell.\n\n\n\n\n\n\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n\n\n\nInstalled kernel version: 1.0.5 \n"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 20\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 20 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 20\nSession ID: 93e16f81-afbb-4200-9d6f-f5ec2b94d5e2\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session 93e16f81-afbb-4200-9d6f-f5ec2b94d5e2 to get into ready status...\nSession 93e16f81-afbb-4200-9d6f-f5ec2b94d5e2 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# important! using python min, max won't work\nfrom pyspark.sql.functions import col, min, max, sum, avg, count, countDistinct, row_number\nfrom pyspark.sql.window import Window\n\n# https://spark.apache.org/docs/latest/sql-ref-datatypes.html\nfrom pyspark.sql.types import StructType, StructField, BooleanType, ByteType, ShortType, IntegerType, StringType, FloatType, DoubleType",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## aisles\nread as csv, save as parquet, then read from parquet",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "aisles_schema = StructType([\n    StructField(\"aisle_id\", IntegerType(), True),\n    StructField(\"aisle\", StringType(), True)\n])\n\n# read from csv in s3\ns3_input_path = \"s3://weikaibucket/data/aisles/aisles.csv\"\naisles = spark.read.csv(s3_input_path, header=True, schema=aisles_schema)\n\n# save as parquet\ns3_output_path = \"s3://weikaibucket/data_parquet/aisles/\"\naisles.write.mode(\"overwrite\").parquet(s3_output_path)\n\n# read parquet\naisles = spark.read.parquet(s3_output_path)\n\naisles.printSchema()\nprint(f'row count: {aisles.count()}')",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": "root\n\n |-- aisle_id: integer (nullable = true)\n\n |-- aisle: string (nullable = true)\n\n\n\nrow count: 134\n"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## departments\nread as csv, save as parquet, then read from parquet",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "departments_schema = StructType([\n    StructField(\"department_id\", IntegerType(), True),\n    StructField(\"department\", StringType(), True)\n])\n\n# read from csv in s3\ndepartments = spark.read.csv(\"s3://weikaibucket/data/departments/departments.csv\", header=True, schema=departments_schema)\n\n# save as parquet\ndepartments.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/departments/\")\n\n# read parquet\ndepartments = spark.read.parquet('s3://weikaibucket/data_parquet/departments')\n\n\ndepartments.printSchema()\n\n# print row number\nprint(f'Row count: {departments.count()}')",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": "root\n\n |-- department_id: integer (nullable = true)\n\n |-- department: string (nullable = true)\n\n\n\nrow count: 21\n"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## products\nread as csv, save as parquet, then read from parquet",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "products_schema = StructType([\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"product_name\", StringType(), True),\n    StructField(\"aisle_id\", IntegerType(), True),\n    StructField(\"department_id\", IntegerType(), True)\n])\n# read from csv in s3\nproducts = spark.read.csv(\"s3://weikaibucket/data/products/products.csv\", header=True, schema=products_schema)\n\n# save as parquet\nproducts.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/products/\")\n\n# read parquet\nproducts = spark.read.parquet('s3://weikaibucket/data_parquet/products/')\n\n\nproducts.printSchema()\n\n# print row number\nprint(f'Row count: {products.count()}')",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- product_id: integer (nullable = true)\n |-- product_name: string (nullable = true)\n |-- aisle_id: integer (nullable = true)\n |-- department_id: integer (nullable = true)\n\nRow count: 49688\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## orders\nread as csv, partition by eval_set, save as parquet, then read from parque",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "orders_schema = StructType([\n    StructField(\"order_id\", IntegerType(), True),\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"eval_set\", StringType(), True),\n    StructField(\"order_number\", IntegerType(), True),\n    StructField(\"order_dow\", ByteType(), True),\n    StructField(\"order_hour_of_day\", ByteType(), True),\n    StructField(\"days_since_prior_order\", FloatType(), True)\n])\n# read from csv in s3\norders = spark.read.csv(\"s3://weikaibucket/data/orders/orders.csv\", header=True, schema=orders_schema)\n\n# save as parquet\norders.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/orders/\")\n\n# read parquet\norders = spark.read.parquet('s3://weikaibucket/data_parquet/orders')\n\n\norders.printSchema()\n\n# print row number\nprint(f'Row count: {orders.count()}')\n",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- order_id: integer (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- eval_set: string (nullable = true)\n |-- order_number: integer (nullable = true)\n |-- order_dow: byte (nullable = true)\n |-- order_hour_of_day: byte (nullable = true)\n |-- days_since_prior_order: float (nullable = true)\n\nRow count: 3421083\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## order_products\nread as csv, save as parquet, then read from parque",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# takes 1 minute to run\norder_products_schema = StructType([\n    StructField(\"order_id\", IntegerType(), True),\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"add_to_cart_order\", IntegerType(), True),\n    StructField(\"reordered\", IntegerType(), True)\n])\norder_products = spark.read.csv(\"s3://weikaibucket/data/order_products/\", header=True, schema=order_products_schema)\n\n# save as parquet\norder_products.write.mode(\"overwrite\").parquet(\"s3://weikaibucket/data_parquet/order_products/\")\n\n# read parquet\norder_products = spark.read.parquet('s3://weikaibucket/data_parquet/order_products')\n\n\norder_products.printSchema()\n\n# print row number\nprint(f'Row count: {order_products.count()}')",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- order_id: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- add_to_cart_order: integer (nullable = true)\n |-- reordered: integer (nullable = true)\n\nRow count: 33819106\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## order_products_prior\nalready save as parquet, just read from parquet",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# read parquet\norder_products_prior = spark.read.parquet('s3://weikaibucket/features/order_products_prior/')\n\n\norder_products_prior.printSchema()\n\n# print row number\nprint(f'Row count: {order_products_prior.count()}')",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- order_id: long (nullable = true)\n |-- user_id: long (nullable = true)\n |-- eval_set: string (nullable = true)\n |-- order_number: long (nullable = true)\n |-- order_dow: long (nullable = true)\n |-- order_hour_of_day: long (nullable = true)\n |-- days_since_prior_order: double (nullable = true)\n |-- product_id: long (nullable = true)\n |-- add_to_cart_order: long (nullable = true)\n |-- reordered: long (nullable = true)\n\nRow count: 32434489\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Q2\n```sql\nCREATE TABLE user_features_1 WITH (\n\texternal_location = 's3://weikaibucket/features/user_features_1/',\n\tformat = 'PARQUET'\n) AS\nSELECT user_id,\n\tMAX(order_number) AS max_order_number,\n\tSUM(days_since_prior_order) AS sum_days_since_prior_order,\n\tAVG(days_since_prior_order) AS avg_days_since_prior_order\nFROM orders\nGROUP BY user_id;\n```",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "user_features_1 = spark.read.parquet('s3://weikaibucket/data_parquet/orders') # read as parquet\n\nuser_features_1 = user_features_1.groupBy('user_id').agg(\n    max('order_number').alias('max_order_number'),\n    sum('days_since_prior_order').alias('sum_days_since_prior_order'),\n    avg('days_since_prior_order').alias('avg_days_since_prior_order')\n)\n\n\nuser_features_1.orderBy('user_id').show(5)\n\n\nprint(f'row count: {user_features_1.count()}')\n\n\ns3_output_path = \"s3://weikaibucket/features/pyspark/user_features_1/\"\nuser_features_1.write.mode(\"overwrite\").parquet(s3_output_path)",
			"metadata": {
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				},
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+----------------+--------------------------+--------------------------+\n|user_id|max_order_number|sum_days_since_prior_order|avg_days_since_prior_order|\n+-------+----------------+--------------------------+--------------------------+\n|      1|              11|                     190.0|                      19.0|\n|      2|              15|                     228.0|        16.285714285714285|\n|      3|              13|                     144.0|                      12.0|\n|      4|               6|                      85.0|                      17.0|\n|      5|               5|                      46.0|                      11.5|\n+-------+----------------+--------------------------+--------------------------+\nonly showing top 5 rows\n\nrow count: 206209\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Q3\n```sql\nCreate Table user_features_2 with (\n\texternal_location = 's3://weikaibucket/features/user_features_2/',\n\tformat = 'parquet'\n) as\nselect user_id,\n\tcount(product_id) as total_number_of_products,\n\tcount(Distinct product_id) as total_number_of_distinct_products,\n\t--整数除法。如果结果是一个小于 1 的小数，那么结果会被截断为 0,所以要乘以1.0变浮点数\n\tsum(if(reordered=1,1,0)) * 1.0 / sum(if(order_number>1,order_number,null)) as reorder_ratio\nfrom order_products_prior\ngroup by user_id;\n```",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "order_products_prior = spark.read.parquet('s3://weikaibucket/features/order_products_prior/') # read as parquet\n\n## Registering data as a temporary view\norder_products_prior.createOrReplaceTempView(\"order_products_prior\")\n\nuser_features_2 = spark.sql('''\nselect user_id,\n\tcount(product_id) as total_number_of_products,\n\tcount(Distinct product_id) as total_number_of_distinct_products,\n\t--整数除法。如果结果是一个小于 1 的小数，那么结果会被截断为 0,所以要乘以1.0变浮点数\n\tsum(if(reordered=1,1,0)) * 1.0 / sum(if(order_number>1,order_number,null)) as reorder_ratio\nfrom order_products_prior\ngroup by user_id;\n''')\n\nuser_features_2.orderBy('user_id').show(5)\n\n\nprint(f'row count: {user_features_2.count()}')\n\n\ns3_output_path = \"s3://weikaibucket/features/pyspark/user_features_2/\"\nuser_features_2.write.mode(\"overwrite\").parquet(s3_output_path)",
			"metadata": {
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				},
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+------------------------+---------------------------------+------------------+\n|user_id|total_number_of_products|total_number_of_distinct_products|     reorder_ratio|\n+-------+------------------------+---------------------------------+------------------+\n|      1|                      59|                               18|0.1213017751479290|\n|      2|                     195|                              102|0.0598455598455598|\n|      3|                      88|                               33|0.1061776061776062|\n|      4|                      18|                               17|0.0208333333333333|\n|      5|                      37|                               23|0.1728395061728395|\n+-------+------------------------+---------------------------------+------------------+\nonly showing top 5 rows\n\nrow count: 206209\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Q4\n```sql\nCreate Table up_features with (\n\texternal_location = 's3://weikaibucket/features/up_features/',\n\tformat = 'parquet'\n) as\nselect user_id,\n\tproduct_id,\n\tcount(order_id) as total_number_of_orders,\n\tmin(order_number) as minimum_order_number,\n\tmax(order_number) as max_order_number,\n\tavg(add_to_cart_order) as avg_add_to_cart_order\nfrom order_products_prior\ngroup by user_id,\n\tproduct_id;\n```",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "order_products_prior.createOrReplaceTempView(\"order_products_prior\")\n\nup_features = spark.sql('''\nselect user_id,\n\tproduct_id,\n\tcount(order_id) as total_number_of_orders,\n\tmin(order_number) as minimum_order_number,\n\tmax(order_number) as max_order_number,\n\tavg(add_to_cart_order) as avg_add_to_cart_order\nfrom order_products_prior\ngroup by user_id,\n\tproduct_id;\n''')\n\nup_features.orderBy('user_id','product_id').show(5)\n\n\nprint(f'row count: {up_features.count()}')\n\n\ns3_output_path = \"s3://weikaibucket/features/pyspark/up_features/\"\nup_features.write.mode(\"overwrite\").parquet(s3_output_path)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+----------+----------------------+--------------------+----------------+---------------------+\n|user_id|product_id|total_number_of_orders|minimum_order_number|max_order_number|avg_add_to_cart_order|\n+-------+----------+----------------------+--------------------+----------------+---------------------+\n|      1|       196|                    10|                   1|              10|                  1.4|\n|      1|     10258|                     9|                   2|              10|   3.3333333333333335|\n|      1|     10326|                     1|                   5|               5|                  5.0|\n|      1|     12427|                    10|                   1|              10|                  3.3|\n|      1|     13032|                     3|                   2|              10|    6.333333333333333|\n+-------+----------+----------------------+--------------------+----------------+---------------------+\nonly showing top 5 rows\n\nrow count: 13307953\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Q5\n```sql\nCreate Table prd_features with (\n\texternal_location = 's3://weikaibucket/features/prd_features/',\n\tformat = 'parquet'\n) as\nselect product_id,\n\tcount(product_id) as count_product,\n\tsum(reordered) as sum_of_reordered,\n\tcount(\n\t\tif (product_seq_table.product_seq_time = 1, 1, null)\n\t) as seq_time1,\n\tcount(\n\t\tif (product_seq_table.product_seq_time = 2, 1, null)\n\t) as seq_time2\nfrom (\n\t\tselect user_id,\n\t\t\tproduct_id,\n\t\t\treordered,\n\t\t\trow_number() over (\n\t\t\t\tpartition by user_id,\n\t\t\t\tproduct_id\n\t\t\t\torder by order_number Asc\n\t\t\t) as product_seq_time\n\t\tfrom order_products_prior\n\t) as product_seq_table\ngroup by product_id;\n```",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "order_products_prior.createOrReplaceTempView(\"order_products_prior\")\n\nprd_features = spark.sql('''\nselect product_id,\n\tcount(product_id) as count_product,\n\tsum(reordered) as sum_of_reordered,\n\tcount(\n\t\tif (product_seq_table.product_seq_time = 1, 1, null)\n\t) as seq_time1,\n\tcount(\n\t\tif (product_seq_table.product_seq_time = 2, 1, null)\n\t) as seq_time2\nfrom (\n\t\tselect user_id,\n\t\t\tproduct_id,\n\t\t\treordered,\n\t\t\trow_number() over (\n\t\t\t\tpartition by user_id,\n\t\t\t\tproduct_id\n\t\t\t\torder by order_number Asc\n\t\t\t) as product_seq_time\n\t\tfrom order_products_prior\n\t) as product_seq_table\ngroup by product_id;\n''')\n\nprd_features.orderBy('product_id').show(5)\n\n\nprint(f'row count: {prd_features.count()}')\n\n\ns3_output_path = \"s3://weikaibucket/features/pyspark/prd_features/\"\nprd_features.write.mode(\"overwrite\").parquet(s3_output_path)",
			"metadata": {
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				},
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-------------+----------------+---------+---------+\n|product_id|count_product|sum_of_reordered|seq_time1|seq_time2|\n+----------+-------------+----------------+---------+---------+\n|         1|         1852|            1136|      716|      276|\n|         2|           90|              12|       78|        8|\n|         3|          277|             203|       74|       36|\n|         4|          329|             147|      182|       64|\n|         5|           15|               9|        6|        4|\n+----------+-------------+----------------+---------+---------+\nonly showing top 5 rows\n\nrow count: 49677\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}